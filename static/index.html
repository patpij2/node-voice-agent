<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Deepgram Voice Agent</title>
</head>
<body>
    <h1>Deepgram Voice Agent Browser Demo</h1>
    <p>This is a demo of the Deepgram Voice Agent. It uses the <a href="https://developers.deepgram.com/reference/voice-agent-api/agent">Deepgram Voice Agent API</a>.</p>
    <p>Please enable your microphone in the browser to start the conversation.</p>
    
    <div id="status" style="padding: 10px; margin: 10px 0; border: 1px solid #ccc; background: #f9f9f9;">
        <strong>Status:</strong> <span id="statusText">Initializing...</span>
    </div>
    
    <button id="testMic" style="padding: 10px; margin: 10px 0; background: #007bff; color: white; border: none; border-radius: 4px; cursor: pointer;">
        Test Microphone
    </button>
    
    <script>
        let socket;
        let mediaStream;
        let audioContext;
        let processor;
        let isConnected = false;
        let audioQueue = []; // Queue for managing incoming audio chunks
        let isPlaying = false; // Flag to track if we're currently playing audio
        let selectedDeviceId;

        function updateStatus(message) {
            const statusText = document.getElementById('statusText');
            if (statusText) {
                statusText.textContent = message;
            }
            console.log('Status:', message);
        }

        async function init() {
            try {
                updateStatus('Getting microphone permission...');
                
                // Create audio context early
                audioContext = new AudioContext({
                    sampleRate: 32000 // Match the server sample rate for better quality
                });

                // Get microphone permission with specific constraints
                const constraints = {
                    audio: {
                        deviceId: selectedDeviceId ? { exact: selectedDeviceId } : undefined,
                        channelCount: 1,
                        sampleRate: 32000,
                        echoCancellation: false,  // Can be toggled
                        noiseSuppression: false,  // Can be toggled
                        autoGainControl: false,   // Can be toggled
                        latency: 0,              // Minimize latency
                        // Advanced constraints for better control
                        googEchoCancellation: false,
                        googAutoGainControl: false,
                        googNoiseSuppression: false,
                        googHighpassFilter: true
                    }
                };
                mediaStream = await navigator.mediaDevices.getUserMedia(constraints);
                updateStatus('Microphone access granted, connecting to server...');

                // Connect to WebSocket server
                socket = new WebSocket('ws://localhost:3000');

                socket.onopen = () => {
                    updateStatus('Connected to server, starting audio stream...');
                    console.log('WebSocket connected, starting audio stream...');
                    isConnected = true;
                    startStreaming();
                };

                socket.onmessage = async (event) => {
                    if (event.data instanceof Blob) {
                        try {
                            const arrayBuffer = await event.data.arrayBuffer();
                            const audioData = new Int16Array(arrayBuffer);
                            // Add to queue instead of playing immediately
                            audioQueue.push(audioData);
                            if (!isPlaying) {
                                playNextInQueue();
                            }
                        } catch (error) {
                            console.error('Error processing audio response:', error);
                        }
                    }
                };

                socket.onerror = (error) => {
                    console.error('WebSocket error:', error);
                    isConnected = false;
                };

                socket.onclose = () => {
                    isConnected = false;
                    stopStreaming();
                };
            } catch (error) {
                console.error('Error initializing:', error);
            }
        }

        async function setupAudioProcessing() {
            const source = audioContext.createMediaStreamSource(mediaStream);

            // Gain control
            const gainNode = audioContext.createGain();

            // Analyzer for volume monitoring
            const analyser = audioContext.createAnalyser();
            analyser.fftSize = 1024;

            // Worklet processor for better performance
            const processor = audioContext.createScriptProcessor(2048, 1, 1);

            // Connect the chain (but don't connect to destination to avoid feedback)
            source
                .connect(gainNode)
                .connect(analyser)
                .connect(processor);

            return { gainNode, analyser, processor };
        }

        function startStreaming() {
            if (!mediaStream || !isConnected) return;

            try {
                console.log('Starting audio stream...');
                updateStatus('Starting audio stream...');
                
                // Ensure audio context is running
                if (audioContext.state === 'suspended') {
                    audioContext.resume().then(() => {
                        console.log('Audio context resumed');
                        updateStatus('Audio context resumed, ready to speak!');
                    });
                } else {
                    updateStatus('Ready to speak!');
                }

                const source = audioContext.createMediaStreamSource(mediaStream);
                console.log('MediaStreamSource created');

                // Create a worklet for better audio processing
                const bufferSize = 2048;
                processor = audioContext.createScriptProcessor(bufferSize, 1, 1);
                console.log('ScriptProcessor created with buffer size:', bufferSize);

                source.connect(processor);
                console.log('Source connected to processor');

                // Create a dummy destination to make the processor work (gain = 0 so no audio output)
                const dummyGain = audioContext.createGain();
                dummyGain.gain.value = 0;
                processor.connect(dummyGain);
                dummyGain.connect(audioContext.destination);
                console.log('Processor connected to dummy destination');

                let lastSendTime = 0;
                const sendInterval = 50; // Send more frequently (every 50ms)
                let audioSent = false;
                let processCount = 0;

                processor.onaudioprocess = (e) => {
                    processCount++;
                    console.log(`Audio process triggered #${processCount}`);
                    
                    const now = Date.now();
                    if (socket?.readyState === WebSocket.OPEN && now - lastSendTime >= sendInterval) {
                        const inputData = e.inputBuffer.getChannelData(0);
                        // Log the first 10 samples for debugging
                        if (!audioSent) {
                            console.log('Mic data (first 10 samples):', Array.from(inputData.slice(0, 10)));
                        }
                        // Always send audio, regardless of silence
                        const pcmData = convertFloatToPcm(inputData);
                        try {
                            socket.send(pcmData.buffer);
                            if (!audioSent) {
                                console.log('First audio packet sent');
                                updateStatus('Audio streaming active - speak now!');
                                audioSent = true;
                            }
                            // Log every send for debugging
                            console.log('Audio packet sent, sample[0]:', inputData[0]);
                            lastSendTime = now;
                        } catch (error) {
                            console.error('Error sending audio:', error);
                        }
                    }
                };
                
                console.log('Audio streaming started');
            } catch (error) {
                console.error('Error starting audio stream:', error);
                updateStatus('Error starting audio stream: ' + error.message);
            }
        }

        function convertFloatToPcm(floatData) {
            const pcmData = new Int16Array(floatData.length);
            for (let i = 0; i < floatData.length; i++) {
                const s = Math.max(-1, Math.min(1, floatData[i]));
                pcmData[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
            }
            return pcmData;
        }

        async function playNextInQueue() {
            if (audioQueue.length === 0) {
                isPlaying = false;
                return;
            }

            isPlaying = true;
            const audioData = audioQueue.shift();

            try {
                // Ensure audio context is running
                if (audioContext.state === 'suspended') {
                    await audioContext.resume();
                }

                // Create buffer with correct sample rate for agent's audio (32000Hz)
                const buffer = audioContext.createBuffer(1, audioData.length, 32000);
                const channelData = buffer.getChannelData(0);

                // Convert Int16 to Float32 with proper scaling
                for (let i = 0; i < audioData.length; i++) {
                    // Normalize to [-1, 1] range
                    channelData[i] = audioData[i] / (audioData[i] >= 0 ? 0x7FFF : 0x8000);
                }

                // Create and configure source
                const source = audioContext.createBufferSource();
                source.buffer = buffer;

                // Create a gain node for volume control
                const gainNode = audioContext.createGain();
                gainNode.gain.value = 0.9; // Slightly reduce volume to avoid clipping

                // Create a low-pass filter to smooth out harsh frequencies
                const lowpassFilter = audioContext.createBiquadFilter();
                lowpassFilter.type = 'lowpass';
                lowpassFilter.frequency.value = 8000; // Cut off harsh high frequencies
                lowpassFilter.Q.value = 0.5; // Gentle rolloff

                // Create a compressor to smooth out dynamics
                const compressor = audioContext.createDynamicsCompressor();
                compressor.threshold.value = -24;
                compressor.knee.value = 30;
                compressor.ratio.value = 12;
                compressor.attack.value = 0.003;
                compressor.release.value = 0.25;

                // Connect nodes for better audio quality
                source.connect(lowpassFilter);
                lowpassFilter.connect(compressor);
                compressor.connect(gainNode);
                gainNode.connect(audioContext.destination);

                // Handle playback completion
                source.onended = () => {
                    playNextInQueue(); // Play next chunk when current one ends
                };

                // Start playback
                source.start(0);
            } catch (error) {
                console.error('Error playing audio:', error);
                isPlaying = false;
                playNextInQueue(); // Try next chunk if current fails
            }
        }

        function stopStreaming() {
            audioQueue = []; // Clear audio queue
            isPlaying = false;
            if (processor) {
                processor.disconnect();
                processor = null;
            }
            if (audioContext) {
                audioContext.close();
                audioContext = null;
            }
            if (mediaStream) {
                mediaStream.getTracks().forEach(track => track.stop());
                mediaStream = null;
            }
            isConnected = false;
        }

        function initializeVolumeMeter(analyser) {
            const bufferLength = analyser.frequencyBinCount;
            const dataArray = new Uint8Array(bufferLength);

            function updateMeter() {
                analyser.getByteFrequencyData(dataArray);
                const average = dataArray.reduce((a, b) => a + b) / bufferLength;
                const volume = Math.min(100, average * 2);
                // Update UI with volume level
                requestAnimationFrame(updateMeter);
            }

            updateMeter();
        }

        async function getAudioDevices() {
            const devices = await navigator.mediaDevices.enumerateDevices();
            return devices.filter(device => device.kind === 'audioinput');
        }

        // Initialize when the page loads
        window.onload = init;

        // Add microphone test functionality
        document.getElementById('testMic').addEventListener('click', async () => {
            try {
                updateStatus('Testing microphone...');
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                const audioContext = new AudioContext();
                const source = audioContext.createMediaStreamSource(stream);
                const analyser = audioContext.createAnalyser();
                analyser.fftSize = 256;
                source.connect(analyser);
                
                const dataArray = new Uint8Array(analyser.frequencyBinCount);
                let testCount = 0;
                
                const testInterval = setInterval(() => {
                    analyser.getByteFrequencyData(dataArray);
                    const average = dataArray.reduce((a, b) => a + b) / dataArray.length;
                    console.log(`Mic test ${testCount}: Average level = ${average}`);
                    updateStatus(`Mic test ${testCount}: Level = ${average}`);
                    testCount++;
                    
                    if (testCount >= 10) {
                        clearInterval(testInterval);
                        stream.getTracks().forEach(track => track.stop());
                        updateStatus('Microphone test complete. Check console for levels.');
                    }
                }, 500);
                
            } catch (error) {
                console.error('Microphone test failed:', error);
                updateStatus('Microphone test failed: ' + error.message);
            }
        });

        // Clean up when the page is closed
        window.onbeforeunload = () => {
            stopStreaming();
            if (socket) {
                socket.close();
            }
        };
    </script>
</body>
</html>